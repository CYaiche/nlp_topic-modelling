{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CYaiche/Machine_Learning/blob/master/projet5/embedding_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Exy2TPsxHFl5"
      },
      "source": [
        "\n",
        "# Supervised topic modeling : NN approach\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    output_dir = \"/content/drive/MyDrive/OpenClassroom/\"\n",
        "    # !pip install bertopic\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    output_dir = \"./output/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "Nebb9sgrHSLV"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.metrics import jaccard_score, average_precision_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Flatten,\n",
        "    Embedding,\n",
        "    Dense,\n",
        "    Input,\n",
        "    Embedding,\n",
        "    GlobalAveragePooling1D,\n",
        ")\n",
        "\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from bertopic import BERTopic\n",
        "# from bertopic.vectorizers import ClassTfidfTransformer\n",
        "# from bertopic.dimensionality import BaseDimensionalityReduction\n",
        "# from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version used :  3.8.10 \n",
            "Tensorflow  :  2.12.0\n"
          ]
        }
      ],
      "source": [
        "print(\"Python version used : \", sys.version[:7])\n",
        "print(\"Tensorflow  : \", tf.version.VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "vawUwgJuG6E0"
      },
      "outputs": [],
      "source": [
        "X_corpus_train = np.load(f\"{output_dir}X_corpus_train.npy\", allow_pickle=True)\n",
        "X_title_train = np.load(f\"{output_dir}X_title_train.npy\", allow_pickle=True)\n",
        "X_body_train = np.load(f\"{output_dir}X_body_train.npy\", allow_pickle=True)\n",
        "\n",
        "X_corpus_test = np.load(f\"{output_dir}X_corpus_test.npy\", allow_pickle=True)\n",
        "X_title_test = np.load(f\"{output_dir}X_title_test.npy\", allow_pickle=True)\n",
        "X_body_test = np.load(f\"{output_dir}X_body_test.npy\", allow_pickle=True)\n",
        "\n",
        "y_train = np.load(f\"{output_dir}y_train.npy\", allow_pickle=True)\n",
        "y_test = np.load(f\"{output_dir}y_test.npy\", allow_pickle=True)\n",
        "\n",
        "label_list = np.load(f\"{output_dir}/label_list.npy\", allow_pickle=True)\n",
        "\n",
        "X_train = [\n",
        "    np.append(X_title_train[i], X_body_train[i]) for i in range(len(X_title_train))\n",
        "]\n",
        "X_test = [\n",
        "    np.append(X_title_test[i], X_body_test[i]) for i in range(len(X_title_test))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J-xWVEn28vC7"
      },
      "outputs": [],
      "source": [
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train_b = mlb.fit_transform(y_train)\n",
        "y_test_b = mlb.transform(y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uS-D3tkeHOPZ"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "Multilayers perceptron (MLP) for multi-label classification\n",
        "\n",
        "loss funstion : binary cross-entropy loss function\n",
        "\n",
        "activation function : ReLU in the hidden layers\n",
        "\n",
        "adam version of stochastic gradient descent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BJxW23-VJqpn"
      },
      "source": [
        "# Embedding preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "if85zWvcMoHq"
      },
      "outputs": [],
      "source": [
        "X_train_list = [ x.tolist() for x in X_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF6stGYZMmbe",
        "outputId": "d7064a51-5c35-4df6-9ef1-f9a17d6e3743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Build & train Word2Vec model ...\n"
          ]
        }
      ],
      "source": [
        "#  Word2Vec model creation\n",
        "w2v_size=300\n",
        "w2v_window=5\n",
        "w2v_min_count=1\n",
        "w2v_epochs=100\n",
        "maxlen = 24 # adapt to length of sentences\n",
        "\n",
        "sentences = X_train_list\n",
        "\n",
        "print(\"Build & train Word2Vec model ...\")\n",
        "w2v_model = Word2Vec(min_count=w2v_min_count, window=w2v_window,\n",
        "                                                vector_size=w2v_size,\n",
        "                                                seed=42,\n",
        "                                                workers=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc3ujOIt5sWq"
      },
      "source": [
        "The reason for separating the trained vectors into KeyedVectors is that if you don’t need the full model state any more (don’t need to continue training), the state can discarded, resulting in a much smaller and faster object that can be mmapped for lightning fast loading and sharing the vectors in RAM between processes:\n",
        "\n",
        "Gensim can also load word vectors in the “word2vec C format”, as a KeyedVectors instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KYe3tvkgMGS7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 39350\n",
            "Word2Vec trained\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    model_vectors = KeyedVectors.load(f\"{output_dir}model_vectors.wv\", mmap=\"r\")\n",
        "except:\n",
        "    w2v_model.build_vocab(sentences)\n",
        "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)\n",
        "    model_vectors = w2v_model.wv\n",
        "    w2v_words = model_vectors.index_to_key\n",
        "    print(\"Vocabulary size: %i\" % len(w2v_words))\n",
        "    print(\"Word2Vec trained\")\n",
        "\n",
        "    model_vectors.save(f\"{output_dir}model_vectors.wv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMBiPMxDQsjW",
        "outputId": "865db21a-6ed0-4fb3-81d2-f8f1652abc97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec<vocab=39350, vector_size=300, alpha=0.025>\n"
          ]
        }
      ],
      "source": [
        "print(w2v_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "t-xufgfQallL"
      },
      "outputs": [],
      "source": [
        "id2word = corpora.Dictionary(X_train)\n",
        "d = dict(zip(label_list, range(0,len(label_list))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "jaLWWIQLOJBS"
      },
      "outputs": [],
      "source": [
        "id2word = corpora.Dictionary(X_train)\n",
        "x_train_ids = [ id2word.doc2idx(tokens) for tokens in X_train]\n",
        "\n",
        "x_test_ids = [ id2word.doc2idx(tokens) for tokens in X_test  ]\n",
        "# Filter out -1 from x_test_ids\n",
        "filtered_x_test_ids = [[word_id for word_id in sentence if word_id != -1] for sentence in x_test_ids]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NCXNAT_RrMA",
        "outputId": "6c994365-07fe-4629-cfd5-91ad74ebc196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_length : 1271\n"
          ]
        }
      ],
      "source": [
        "max_length = np.max([ len(x) for x in x_train_ids])\n",
        "print(f\"max_length : {max_length}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ETr4guPjpjau"
      },
      "source": [
        "# Embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZM-9CW-qch7",
        "outputId": "9235cfae-1754-4aec-eeac-0380f83d3416"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(model_vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "087wFGULpmS9",
        "outputId": "5656f95f-8d51-456d-e982-8ab9e702e207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding matrix: (39350, 300)\n"
          ]
        }
      ],
      "source": [
        "word2vec_vocab_size = len(model_vectors.key_to_index)\n",
        "w2vec_dim = len(model_vectors[0])\n",
        "word2vec_embedding_matrix = np.zeros((word2vec_vocab_size, w2vec_dim))\n",
        "\n",
        "for word in model_vectors.key_to_index:\n",
        "    embedding_vector = model_vectors[word]\n",
        "    if embedding_vector is not None:\n",
        "        idx = model_vectors.key_to_index[word]\n",
        "        word2vec_embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "print(\"Embedding matrix: %s\" % str(word2vec_embedding_matrix.shape))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nfo8-bOfbMLW"
      },
      "source": [
        "# Apply padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "r91-eP4gY51k"
      },
      "outputs": [],
      "source": [
        "x_train_pad = pad_sequences(x_train_ids, maxlen=max_length, padding='pre',value=0)\n",
        "x_test_pad = pad_sequences(filtered_x_test_ids, maxlen=max_length, padding='pre',value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbkX2vkYQWml",
        "outputId": "3937b14a-5ee8-48ad-959f-f435e1b6d576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 1271, 300)         11805000  \n",
            "                                                                 \n",
            " global_average_pooling1d_15  (None, 300)              0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 30)                9030      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,814,030\n",
            "Trainable params: 11,814,030\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "# model.add(Input(shape=(1,max_length),dtype='float64'))\n",
        "model.add(Embedding( word2vec_vocab_size, w2vec_dim,weights=[word2vec_embedding_matrix],  input_length=max_length))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(30,activation='sigmoid'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "GEpi_Spe3KSF"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp97pRuR8bu-",
        "outputId": "9942339f-43c2-41ba-92d0-b863a750ae20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50/50 [==============================] - 8s 149ms/step - loss: 0.3093 - accuracy: 0.1163\n",
            "Epoch 2/5\n",
            "50/50 [==============================] - 8s 159ms/step - loss: 0.1836 - accuracy: 0.2356\n",
            "Epoch 3/5\n",
            "50/50 [==============================] - 8s 163ms/step - loss: 0.1822 - accuracy: 0.2200\n",
            "Epoch 4/5\n",
            "50/50 [==============================] - 8s 158ms/step - loss: 0.1824 - accuracy: 0.1694\n",
            "Epoch 5/5\n",
            "50/50 [==============================] - 8s 150ms/step - loss: 0.1823 - accuracy: 0.2525\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1a526d0deb0>"
            ]
          },
          "execution_count": 224,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train_pad, y_train_b, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O8ofX6_a__Ne",
        "outputId": "1bc97b10-fccb-414e-bea5-dec4ea5a2e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 0s 13ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(x_test_pad)\n",
        "y_pred_nn = (y_pred > 0.01).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "ezkkFTFV9e8Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "precision_w2v      = average_precision_score(y_test_b, y_pred_nn, average='micro')\n",
        "jaccard_score_w2v = jaccard_score(y_test_b, y_pred_nn, average='micro')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jl4XyvqgK4h0"
      },
      "source": [
        "The vocab is the number of unique words in my train data.\n",
        "The size is the dimension in output of my embedding."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# USE : Universal Sentence Encoder \n",
        "encodes into high-dimensional vectors, trained on variety of data and output 512 dimensional vector"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# apply padding to words "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_use_embedding = embed(X_corpus_train)\n",
        "X_test_use_embedding = embed(X_corpus_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_25 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 30)                3870      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 168,094\n",
            "Trainable params: 168,094\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "use_model = Sequential()\n",
        "use_model.add(Dense(256, activation='relu', input_dim=512))\n",
        "use_model.add(Dense(128, activation='relu'))\n",
        "use_model.add(Dense(30,activation='sigmoid'))\n",
        "# Compile the model\n",
        "use_model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "use_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50/50 [==============================] - 2s 3ms/step - loss: 0.3431 - accuracy: 0.0862\n",
            "Epoch 2/5\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.3925\n",
            "Epoch 3/5\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1457 - accuracy: 0.5056\n",
            "Epoch 4/5\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1210 - accuracy: 0.5612\n",
            "Epoch 5/5\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1034 - accuracy: 0.6112\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1a5255af760>"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_model.fit(X_train_use_embedding, y_train_b, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1/13 [=>............................] - ETA: 0s"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 0s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred_use = use_model.predict(X_test_use_embedding)\n",
        "y_pred_use = (y_pred_use > 0.01).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "precision_use      = average_precision_score(y_test_b, y_pred_use, average='micro')\n",
        "jaccard_score_use = jaccard_score(y_test_b, y_pred_use, average='micro')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bertopic'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[235], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbertopic\u001b[39;00m \u001b[39mimport\u001b[39;00m BERTopic\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbertopic\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorizers\u001b[39;00m \u001b[39mimport\u001b[39;00m ClassTfidfTransformer\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbertopic\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdimensionality\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseDimensionalityReduction\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'"
          ]
        }
      ],
      "source": [
        "from bertopic import BERTopic\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from bertopic.dimensionality import BaseDimensionalityReduction\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Get labeled data\n",
        "data = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))\n",
        "docs = data['data']\n",
        "y = data['target']\n",
        "\n",
        "# Skip over dimensionality reduction, replace cluster model with classifier,\n",
        "# and reduce frequent words while we are at it.\n",
        "empty_dimensionality_model = BaseDimensionalityReduction()\n",
        "clf = LogisticRegression()\n",
        "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "\n",
        "# Create a fully supervised BERTopic instance\n",
        "topic_model= BERTopic(\n",
        "        umap_model=empty_dimensionality_model,\n",
        "        hdbscan_model=clf,\n",
        "        ctfidf_model=ctfidf_model\n",
        ")\n",
        "topics, probs = topic_model.fit_transform(docs, y=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "OzX5AL7uKDfu"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SentenceTransformer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39mdistilbert-base-nli-mean-tokens\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m embeddings_corpus \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(X_corpus_train, show_progress_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m embeddings_titles \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(X_title_train, show_progress_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
          ]
        }
      ],
      "source": [
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "embeddings_corpus = model.encode(X_body_train, show_progress_bar=True)\n",
        "embeddings_titles = model.encode(X_title_train, show_progress_bar=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Skip over dimensionality reduction, replace cluster model with classifier,\n",
        "# and reduce frequent words while we are at it.\n",
        "empty_dimensionality_model = BaseDimensionalityReduction()\n",
        "clf = LogisticRegression()\n",
        "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "\n",
        "# Create a fully supervised BERTopic instance\n",
        "topic_model= BERTopic(\n",
        "        umap_model=empty_dimensionality_model,\n",
        "        hdbscan_model=clf,\n",
        "        ctfidf_model=ctfidf_model\n",
        ")\n",
        "topics, probs = topic_model.fit_transform(docs, y=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEq4gPPrJ83U"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdTQlZa_KD_b"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "model.wv.similarity('splint','tableview')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAG5dW41KGSb"
      },
      "outputs": [],
      "source": [
        "X_train_embedding = model.wv[X_train]\n",
        "X_test_embedding = model.wv[X_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVB4Ecs7Jp5c"
      },
      "outputs": [],
      "source": [
        "# Embedding preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjBTvm_QHUJM"
      },
      "outputs": [],
      "source": [
        "word_model = Sequential()\n",
        "word_model.add(Embedding(vocab_size, embed_size,\n",
        "                         embeddings_initializer=\"glorot_uniform\",\n",
        "                         input_length=1))\n",
        "word_model.add(Reshape((embed_size, )))\n",
        "\n",
        "context_model = Sequential()\n",
        "context_model.add(Embedding(vocab_size, embed_size,\n",
        "                  embeddings_initializer=\"glorot_uniform\",\n",
        "                  input_length=1))\n",
        "context_model.add(Reshape((embed_size,)))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Merge([word_model, context_model], mode=\"dot\"))\n",
        "model.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg_precision_scores = [precision_w2v, precision_use,]\n",
        "jaccard_scores = [jaccard_score_w2v, jaccard_score_use]\n",
        "result = pd.DataFrame(\n",
        "    {\n",
        "        'average_precision_scores' : avg_precision_scores,\n",
        "        'jaccard_scores' : jaccard_scores\n",
        "    }, index = ['Word2Vec','USE']\n",
        ")\n",
        "result.to_csv(f\"{output_dir}result_supervised2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>average_precision_scores</th>\n",
              "      <th>jaccard_scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Word2Vec</th>\n",
              "      <td>0.04850</td>\n",
              "      <td>0.048500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>USE</th>\n",
              "      <td>0.09077</td>\n",
              "      <td>0.091183</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          average_precision_scores  jaccard_scores\n",
              "Word2Vec                   0.04850        0.048500\n",
              "USE                        0.09077        0.091183"
            ]
          },
          "execution_count": 234,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMRwjeuBeVraeS78MTKQby9",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
