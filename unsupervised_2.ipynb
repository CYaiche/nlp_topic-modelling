{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of LDA on X_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import gensim\n",
    "from  tm_common import *\n",
    "\n",
    "from gensim.test.utils import datapath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB, output_dir = tm_get_working_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tm_load_train_test_set(output_dir)\n",
    "X_train, y_train       = tm_get_subset(X_train, y_train)\n",
    "X_test, y_test       = tm_get_subset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_b, y_test_b = tm_multilabel_binarizer(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6182\n",
      "1545\n"
     ]
    }
   ],
   "source": [
    "print( len(X_train) )\n",
    "print( len(y_test)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(X_train)\n",
    "train_corpus = [ id2word.doc2bow(text) for text in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model_tunned =  gensim.models.LdaModel.load(\"./model/lda_model_tunned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [ id2word.doc2bow(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"CDUMessage\" + 0.000*\"ymm1\" + 0.000*\"vaddps\" + 0.000*\"YMMWORD\" + 0.000*\"vmulps\" + 0.000*\"0.000e+000\" + 0.000*\"GFLOPs\" + 0.000*\":CDUMessageType\"'),\n",
       " (2,\n",
       "  '0.001*\"UIKit\" + 0.001*\"__m128i\" + 0.001*\"0x90909090\" + 0.000*\"CoreFoundation\" + 0.000*\"fetchRequest\" + 0.000*\"libsystem_c.dylib\" + 0.000*\"NSPredicate\" + 0.000*\"NSFetchRequest\" + 0.000*\"InstantaneousBeatsPerMinute\" + 0.000*\"bpm=\"'),\n",
       " (13,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"ymm1\" + 0.000*\"CDUMessage\" + 0.000*\"YMMWORD\" + 0.000*\"vaddps\" + 0.000*\"vmulps\" + 0.000*\"0.000e+000\" + 0.000*\"GFLOPs\" + 0.000*\":CDUMessageType\"'),\n",
       " (10,\n",
       "  '0.002*\"work\" + 0.002*\"read\" + 0.002*\"template\" + 0.002*\"script\" + 0.002*\"file\" + 0.002*\"possible\" + 0.002*\"thing\" + 0.002*\"size\" + 0.002*\"running\" + 0.002*\"query\"'),\n",
       " (3,\n",
       "  '0.005*\"File\" + 0.001*\"Traceback\" + 0.001*\"models.CharField\" + 0.000*\"models.ForeignKey\" + 0.000*\"models.Model\" + 0.000*\"null=True\" + 0.000*\"kwargs\" + 0.000*\"__call__\" + 0.000*\"recent\" + 0.000*\"models.IntegerField\"'),\n",
       " (16,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"CDUMessage\" + 0.000*\"ymm1\" + 0.000*\"YMMWORD\" + 0.000*\"vaddps\" + 0.000*\"vmulps\" + 0.000*\"GFLOPs\" + 0.000*\"0.000e+000\" + 0.000*\":CDUMessageType\"'),\n",
       " (25,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"ymm1\" + 0.000*\"CDUMessage\" + 0.000*\"YMMWORD\" + 0.000*\"vaddps\" + 0.000*\"vmulps\" + 0.000*\"0.000e+000\" + 0.000*\"GFLOPs\" + 0.000*\"linkHtml\"'),\n",
       " (20,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"ymm1\" + 0.000*\"vaddps\" + 0.000*\"YMMWORD\" + 0.000*\"vmulps\" + 0.000*\"CDUMessage\" + 0.000*\"0.000e+000\" + 0.000*\"GFLOPs\" + 0.000*\"linkHtml\"'),\n",
       " (27,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"CDUMessage\" + 0.000*\"ymm1\" + 0.000*\"YMMWORD\" + 0.000*\"vaddps\" + 0.000*\"vmulps\" + 0.000*\"GFLOPs\" + 0.000*\"0.000e+000\" + 0.000*\":CDUMessageType\"'),\n",
       " (26,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"CDUMessage\" + 0.000*\"ymm1\" + 0.000*\"YMMWORD\" + 0.000*\"vaddps\" + 0.000*\"vmulps\" + 0.000*\"0.000e+000\" + 0.000*\"GFLOPs\" + 0.000*\":CDUMessageType\"'),\n",
       " (8,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"ymm1\" + 0.000*\"CDUMessage\" + 0.000*\"YMMWORD\" + 0.000*\"vaddps\" + 0.000*\"vmulps\" + 0.000*\"GFLOPs\" + 0.000*\"0.000e+000\" + 0.000*\":CDUMessageType\"'),\n",
       " (24,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"CDUMessage\" + 0.000*\"ymm1\" + 0.000*\"YMMWORD\" + 0.000*\"vaddps\" + 0.000*\"vmulps\" + 0.000*\"0.000e+000\" + 0.000*\"GFLOPs\" + 0.000*\":CDUMessageType\"'),\n",
       " (12,\n",
       "  '0.005*\"Override\" + 0.002*\"activity\" + 0.002*\"final\" + 0.002*\"fragment\" + 0.001*\"Intent\" + 0.001*\"extends\" + 0.001*\"View\" + 0.001*\"savedInstanceState\" + 0.001*\"Activity\" + 0.001*\"Bundle\"'),\n",
       " (21,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"CDUMessage\" + 0.000*\"ymm1\" + 0.000*\"vaddps\" + 0.000*\"YMMWORD\" + 0.000*\"vmulps\" + 0.000*\"0.000e+000\" + 0.000*\"GFLOPs\" + 0.000*\"linkHtml\"'),\n",
       " (14,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"CDUMessage\" + 0.000*\"ymm1\" + 0.000*\"vaddps\" + 0.000*\"YMMWORD\" + 0.000*\"vmulps\" + 0.000*\"0.000e+000\" + 0.000*\"GFLOPs\" + 0.000*\":CDUMessageType\"'),\n",
       " (15,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"CDUMessage\" + 0.000*\"ymm1\" + 0.000*\"YMMWORD\" + 0.000*\"vaddps\" + 0.000*\"vmulps\" + 0.000*\"0.000e+000\" + 0.000*\"GFLOPs\" + 0.000*\":CDUMessageType\"'),\n",
       " (1,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"CDUMessage\" + 0.000*\"ymm1\" + 0.000*\"YMMWORD\" + 0.000*\"vaddps\" + 0.000*\"vmulps\" + 0.000*\"GFLOPs\" + 0.000*\"0.000e+000\" + 0.000*\":CDUMessageType\"'),\n",
       " (5,\n",
       "  '0.002*\"Loss_G\" + 0.002*\"Loss_D\" + 0.001*\"loss\" + 0.001*\"patch_size\" + 0.001*\"epoch\" + 0.001*\"training\" + 0.001*\"np.array\" + 0.000*\"np.ndarray\" + 0.000*\"np.exp\" + 0.000*\"weight\"'),\n",
       " (7,\n",
       "  '0.000*\"ymm9\" + 0.000*\"0x90909090\" + 0.000*\"CDUMessage\" + 0.000*\"ymm1\" + 0.000*\"vaddps\" + 0.000*\"YMMWORD\" + 0.000*\"vmulps\" + 0.000*\"GFLOPs\" + 0.000*\"0.000e+000\" + 0.000*\":CDUMessageType\"'),\n",
       " (28,\n",
       "  '0.001*\"Extent1\" + 0.001*\"JOIN\" + 0.001*\"INNER\" + 0.000*\"Project1\" + 0.000*\"DATEADD\" + 0.000*\"CCP1\" + 0.000*\"TEntity\" + 0.000*\"GroupBy1\" + 0.000*\"VenuePanels.PanelID\" + 0.000*\"LEFT\"')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_tunned.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue : the class are not the same as supervised ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'float' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m y_pred \u001b[39m=\u001b[39m lda_model_tunned[train_corpus[:\u001b[39m100\u001b[39m]]\n\u001b[1;32m----> 2\u001b[0m threshold_test_use \u001b[39m=\u001b[39m tm_test_threshold(y_train_b[:\u001b[39m100\u001b[39;49m], y_pred)  \n\u001b[0;32m      3\u001b[0m tm_plot_threshold_test(threshold_test_use)\n",
      "File \u001b[1;32mc:\\dev\\topic_modelling\\tm_common.py:125\u001b[0m, in \u001b[0;36mtm_test_threshold\u001b[1;34m(y_train_b, y_pred)\u001b[0m\n\u001b[0;32m    123\u001b[0m test_thr \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mthr\n\u001b[0;32m    124\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m test_thr : \n\u001b[1;32m--> 125\u001b[0m     y_pred_t \u001b[39m=\u001b[39m (y_pred \u001b[39m>\u001b[39;49m t )\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m    126\u001b[0m     prec      \u001b[39m=\u001b[39m average_precision_score(y_train_b, y_pred_t, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    127\u001b[0m     jacc \u001b[39m=\u001b[39m jaccard_score(y_train_b, y_pred_t, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'float' and 'list'"
     ]
    }
   ],
   "source": [
    "y_pred = lda_model_tunned[train_corpus[:100]]\n",
    "threshold_test_use = tm_test_threshold(y_train_b[:100], y_pred)  \n",
    "tm_plot_threshold_test(threshold_test_use)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_corpus \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(train_corpus)\n\u001b[1;32m----> 2\u001b[0m topic_term_dists \u001b[39m=\u001b[39m train_corpus \u001b[39m/\u001b[39m train_corpus\u001b[39m.\u001b[39;49msum(axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)[:, \u001b[39mNone\u001b[39;00m]\n",
      "File \u001b[1;32mc:\\dev\\topic_modelling\\lib\\site-packages\\numpy\\core\\_methods.py:48\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     47\u001b[0m          initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "train_corpus = np.array(train_corpus)\n",
    "topic_term_dists = train_corpus / train_corpus.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "\n * Not all rows (distributions) in topic_term_dists sum to 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m pyLDAvis\u001b[39m.\u001b[39menable_notebook()\n\u001b[1;32m----> 3\u001b[0m LDAvis_prepared \u001b[39m=\u001b[39m pyLDAvis\u001b[39m.\u001b[39;49mgensim\u001b[39m.\u001b[39;49mprepare(lda_model_tunned, train_corpus, id2word)\n\u001b[0;32m      5\u001b[0m LDAvis_prepared\n",
      "File \u001b[1;32mc:\\dev\\topic_modelling\\lib\\site-packages\\pyLDAvis\\gensim.py:123\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39mthe data structures needed for the visualization.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mSee `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m opts \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[1;32m--> 123\u001b[0m \u001b[39mreturn\u001b[39;00m vis_prepare(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts)\n",
      "File \u001b[1;32mc:\\dev\\topic_modelling\\lib\\site-packages\\pyLDAvis\\_prepare.py:408\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[0;32m    406\u001b[0m doc_lengths \u001b[39m=\u001b[39m _series_with_name(doc_lengths, \u001b[39m'\u001b[39m\u001b[39mdoc_length\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    407\u001b[0m vocab \u001b[39m=\u001b[39m _series_with_name(vocab, \u001b[39m'\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 408\u001b[0m _input_validate(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)\n\u001b[0;32m    409\u001b[0m R \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(R, \u001b[39mlen\u001b[39m(vocab))\n\u001b[0;32m    411\u001b[0m topic_freq \u001b[39m=\u001b[39m doc_topic_dists\u001b[39m.\u001b[39mmul(doc_lengths, axis\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\dev\\topic_modelling\\lib\\site-packages\\pyLDAvis\\_prepare.py:69\u001b[0m, in \u001b[0;36m_input_validate\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     67\u001b[0m res \u001b[39m=\u001b[39m _input_check(\u001b[39m*\u001b[39margs)\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m res:\n\u001b[1;32m---> 69\u001b[0m     \u001b[39mraise\u001b[39;00m ValidationError(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39m'\u001b[39m\u001b[39m * \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m s \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m res]))\n",
      "\u001b[1;31mValidationError\u001b[0m: \n * Not all rows (distributions) in topic_term_dists sum to 1."
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model_tunned, train_corpus, id2word)\n",
    "\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic_modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
